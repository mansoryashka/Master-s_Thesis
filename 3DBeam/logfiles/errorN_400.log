nohup: ignoring input
/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in TanhBackwardBackward0. Traceback of forward call that caused the error:
  File "/storage/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/traceback.py", line 213, in format_stack
    return format_list(extract_stack(f, limit=limit))
 (Triggered internally at  /dev/shm/PyTorch/1.12.0/foss-2022a-CUDA-11.7.0/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:102.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: 

Previous calculation was induced by TanhBackward0. Traceback of forward call that induced the previous calculation:
  File "/itf-fi-ml/home/mansurd/Master/3DBeam/DemBeam3D.py", line 527, in <module>
    U_norms_i, losses_i = train_and_evaluate(Ns=Ns, lrs=lrs, num_neurons=num_neurons, num_layers=num_layers, num_epochs=num_epochs, eval_data=[x_eval, y_eval, z_eval])
  File "/itf-fi-ml/home/mansurd/Master/3DBeam/DemBeam3D.py", line 342, in train_and_evaluate
    DemBeam.train_model(domain, dirichlet, neumann, shape, LHD, lr=lr, max_it=max_it, epochs=num_epochs, eval_data=eval_data)
  File "/itf-fi-ml/home/mansurd/Master/3DBeam/../DEM.py", line 119, in train_model
    optimizer.step(closure)
  File "/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/optim/optimizer.py", line 109, in wrapper
    return func(*args, **kwargs)
  File "/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/optim/lbfgs.py", line 437, in step
    loss = float(closure())
  File "/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/itf-fi-ml/home/mansurd/Master/3DBeam/../DEM.py", line 83, in closure
    u_pred = self.getU(self.model, x)
  File "/itf-fi-ml/home/mansurd/Master/3DBeam/../DEM.py", line 155, in getU
    u = model(x).to(dev)
  File "/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/itf-fi-ml/home/mansurd/Master/3DBeam/../DEM.py", line 39, in forward
    x = torch.tanh(torch.nn.functional.linear(x, layer.weight.clone(), layer.bias))
 (Triggered internally at  /dev/shm/PyTorch/1.12.0/foss-2022a-CUDA-11.7.0/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:106.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/itf-fi-ml/home/mansurd/Master/3DBeam/DemBeam3D.py", line 527, in <module>
    U_norms_i, losses_i = train_and_evaluate(Ns=Ns, lrs=lrs, num_neurons=num_neurons, num_layers=num_layers, num_epochs=num_epochs, eval_data=[x_eval, y_eval, z_eval])
  File "/itf-fi-ml/home/mansurd/Master/3DBeam/DemBeam3D.py", line 342, in train_and_evaluate
    DemBeam.train_model(domain, dirichlet, neumann, shape, LHD, lr=lr, max_it=max_it, epochs=num_epochs, eval_data=eval_data)
  File "/itf-fi-ml/home/mansurd/Master/3DBeam/../DEM.py", line 119, in train_model
    optimizer.step(closure)
  File "/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/optim/optimizer.py", line 109, in wrapper
    return func(*args, **kwargs)
  File "/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/optim/lbfgs.py", line 437, in step
    loss = float(closure())
  File "/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/itf-fi-ml/home/mansurd/Master/3DBeam/../DEM.py", line 110, in closure
    loss.backward(retain_graph=True)
  File "/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/storage/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 10.75 GiB total capacity; 10.20 GiB already allocated; 51.56 MiB free; 10.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
